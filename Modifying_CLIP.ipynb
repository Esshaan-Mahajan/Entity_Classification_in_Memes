{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Interacting with CLIP\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z06JJ9OP2G7",
        "outputId": "2af0a933-63f1-47e0-eb45-f7b6a1f5a8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9LO5nG0glDD"
      },
      "outputs": [],
      "source": [
        "root_path = 'gdrive/My Drive/iiitd_research'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLrEEWoYSUEA",
        "outputId": "8461ccd1-0d41-43a1-c579-d50e5fd4bae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'gdrive', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir())\n",
        "os.chdir('gdrive/My Drive/iiitd_research')\n",
        "#!unzip 'constraint22_dataset_covid19.zip'\n",
        "# print(os.getcwd())\n",
        "# print(os.listdir('./content/covid_meme_data'))\n",
        "\n",
        "\n",
        "#os.chdir('/content/view?usp=sharing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBeEU1HdidWz"
      },
      "outputs": [],
      "source": [
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tztpvwBcif2e"
      },
      "outputs": [],
      "source": [
        "#!unzip 'constraint22_dataset_covid19.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip 'constraint22_dataset_uspolitics.zip'\n",
        "\n"
      ],
      "metadata": {
        "id": "ehUQUpKRGrgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDjrxKm1irsr",
        "outputId": "003821e2-e5d8-42ba-b677-f5ca5d28f805"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['val.jsonl', 'train.jsonl', 'train2.jsonl', 'val2.jsonl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "os.listdir('annotations')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7KMYrgDqNuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting data for training"
      ],
      "metadata": {
        "id": "zsaW_g1QGS1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCmVOmwLjJT1",
        "outputId": "325f42aa-6d63-4421-f2f5-77ce983c44e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OCR': 'MARIA BUTINA USED SEX\\nTO INFILTRATE THE NRA\\n& THE REPUBLICAN PARTY\\nSO THERE MAY BE SEX TAPES\\nOF MITCH MCCONNELL!\\n', 'image': 'memes_4544.png', 'hero': [], 'villain': ['mitch mcconnell'], 'victim': [], 'other': ['national rifle association (nra)', 'nra', 'maria butina', 'republican party']}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "with open('./annotations/val2.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "print(data[4])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xIlRge4rMmz",
        "outputId": "e4f7732e-83cd-4b71-c611-ca3297f9457a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_new = []\n",
        "k=0\n",
        "for i in data :\n",
        "\n",
        "  i['entities']=i['hero']+i['villain']+i['victim']+i['other']\n",
        "  #if len(i[\"OCR\"])<=1500:\n",
        "  for en in i['entities']:\n",
        "      new_dict={}\n",
        "\n",
        "      new_dict['image'] = i['image']\n",
        "      new_dict['OCR'] = i['OCR']\n",
        "      new_dict['entity'] = en\n",
        "      if en in i['hero']:\n",
        "        new_dict['label'] =0 #'hero'\n",
        "      elif en in i['villain'] :\n",
        "        new_dict['label'] =1 #'villain'\n",
        "      elif en in i['victim'] :\n",
        "        new_dict['label'] =2 #'victim'\n",
        "      else:\n",
        "        new_dict['label'] =3 #'other'\n",
        "\n",
        "      data_new.append(new_dict)\n",
        "  k=k+1"
      ],
      "metadata": {
        "id": "1Z3j_d0wpe7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_new))\n",
        "print(k)\n",
        "print(data_new[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJoUSsANudFR",
        "outputId": "61da54f6-1645-4a2b-fcdb-71ed9f5990b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "823\n",
            "300\n",
            "[{'image': 'covid_memes_2457.png', 'OCR': \"Herman Caino\\nOTHEHermancain\\nMasks will not be mandatory for the event,\\nwhich will be attended by President\\nTrump. PEOPLE ARE FED UP!\\nHERE'S SOME COVID FOR YOU\\nAND VOU, AND YOU, AND YOU,\\n\", 'entity': 'herman cain', 'label': 1}, {'image': 'covid_memes_2457.png', 'OCR': \"Herman Caino\\nOTHEHermancain\\nMasks will not be mandatory for the event,\\nwhich will be attended by President\\nTrump. PEOPLE ARE FED UP!\\nHERE'S SOME COVID FOR YOU\\nAND VOU, AND YOU, AND YOU,\\n\", 'entity': 'donald trump', 'label': 3}, {'image': 'covid_memes_2457.png', 'OCR': \"Herman Caino\\nOTHEHermancain\\nMasks will not be mandatory for the event,\\nwhich will be attended by President\\nTrump. PEOPLE ARE FED UP!\\nHERE'S SOME COVID FOR YOU\\nAND VOU, AND YOU, AND YOU,\\n\", 'entity': 'covid', 'label': 3}, {'image': 'covid_memes_2457.png', 'OCR': \"Herman Caino\\nOTHEHermancain\\nMasks will not be mandatory for the event,\\nwhich will be attended by President\\nTrump. PEOPLE ARE FED UP!\\nHERE'S SOME COVID FOR YOU\\nAND VOU, AND YOU, AND YOU,\\n\", 'entity': 'coronavirus', 'label': 3}, {'image': 'covid_memes_2457.png', 'OCR': \"Herman Caino\\nOTHEHermancain\\nMasks will not be mandatory for the event,\\nwhich will be attended by President\\nTrump. PEOPLE ARE FED UP!\\nHERE'S SOME COVID FOR YOU\\nAND VOU, AND YOU, AND YOU,\\n\", 'entity': 'mask', 'label': 3}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max=0\n",
        "\n",
        "avg=0\n",
        "for i in data_new:\n",
        "  avg+=len(i['OCR'])\n",
        "  if len(i['OCR'])>max:\n",
        "    max=len(i['OCR'])\n",
        "\n",
        "\n",
        "print(max,avg/10280)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoGgEAlgQhEf",
        "outputId": "8ce9aba7-bfad-4c81-f18a-acf989e696d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1069 146.6580739299611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4HauCJvqdnB",
        "outputId": "e78bb407-49e7-46d3-cd73-feaf3a1dd4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OCR': 'Occupy Danocrats\\na at 12\\nDREAKNG: Barack Cbaria andoruas Jos lidan for president wtt\\nRALLIANT speech\\nFolon Cetuny Desnouta ler reare.\\nBREAKING:\\nBarack Obama endorses\\nJoe Biden for president\\nJoe Biden for Presideht of the United States.\\nK Corers a sras. 2a Vies\\nO comment\\nA Shere\\n', 'image': 'covid_memes_622.png', 'hero': ['joe biden'], 'villain': [], 'victim': [], 'other': ['barack obama'], 'entities': ['joe biden', 'barack obama']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDQHwaJfqqrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm_EWpQajarq"
      },
      "outputs": [],
      "source": [
        "#os.listdir('images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "3ef811d2-7fa5-43f5-a5d7-9419cfd6f13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 946 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yhoanb11\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-yhoanb11\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=7763ebce1e426f09fdf5f2764e56be78fa43b35bd7d0ea709448c6529e66a50c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qwng5phz/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "C1hkDT38hSaP",
        "outputId": "eb8f1ef0-ee91-40e0-b8f5-63adeb4eb616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 1.11.0+cu113\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.11.0+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "torch.__version__#.split(\".\") #>= [\"1\", \"7\", \"1\"], \"PyTorch 1.7.1 or later is required\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, gc\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "9iWLJZJM8e-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFS29hnhlY4",
        "outputId": "89fe65b1-a23a-47a6-cc56-8515861c52bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "42041ea6-de50-4adf-aa2c-c4f1ad16bb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 890M/890M [00:07<00:00, 126MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 427,616,513\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-L/14\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21slhZGCqANb"
      },
      "source": [
        "# Image Preprocessing\n",
        "\n",
        "We resize the input images and center-crop them to conform with the image resolution that the model expects. Before doing so, we will normalize the pixel intensity using the dataset mean and standard deviation.\n",
        "\n",
        "The second return value from `clip.load()` contains a torchvision `Transform` that performs this preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6cpiIFHp9N6",
        "outputId": "f03bfeed-f179-4b81-dfd4-3773cf2e4d83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7f90ee7e65f0>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwSB5jZki3Cj"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "We use a case-insensitive tokenizer, which can be invoked using `clip.tokenize()`. By default, the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGom156-i2kL",
        "outputId": "16842e29-ee9c-4ca5-a6d0-4ca6faa31e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "clip.tokenize(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8ARJVqBJXs"
      },
      "source": [
        "# Setting up input images and texts\n",
        "\n",
        "We are going to feed 8 example images and their textual descriptions to the model, and compare the similarity between the corresponding features.\n",
        "\n",
        "The tokenizer is case-insensitive, and we can freely give any suitable textual descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t1hyArZQalC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gOYUVuhQhph"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMc1AXzBlhzm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# images in skimage to use and their textual descriptions\n",
        "# descriptions = {\n",
        "#     \"page\": \"a page of text about segmentation\",\n",
        "#     \"chelsea\": \"a facial photo of a tabby cat\",\n",
        "#     \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
        "#     \"rocket\": \"a rocket standing on a launchpad\",\n",
        "#     \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
        "#     \"camera\": \"a person looking at a camera on a tripod\",\n",
        "#     \"horse\": \"a black-and-white silhouette of a horse\",\n",
        "#     \"coffee\": \"a cup of coffee on a saucer\"\n",
        "# }\n",
        "# descriptions = {}\n",
        "# for image_desc in data:\n",
        "#   if len(image_desc[\"OCR\"])<77:\n",
        "#     descriptions[image_desc[\"image\"]] = image_desc[\"OCR\"]\n",
        "\n",
        "# print(descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6ix0IpjsJvmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkBPFMCLmpVN"
      },
      "outputs": [],
      "source": [
        "#(os.listdir('images'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSSrLY185jSf"
      },
      "outputs": [],
      "source": [
        "original_images = []\n",
        "images = []\n",
        "ocr = []\n",
        "entity=[]\n",
        "labels=[]\n",
        "\n",
        "n=0\n",
        "temp=''\n",
        "for i in data_new:\n",
        "    # n+=1\n",
        "    # if n==1004:\n",
        "    #   break\n",
        "\n",
        "    if i['image'] not in os.listdir('images'): #or i['image'][0]=='c':\n",
        "        continue\n",
        "\n",
        "\n",
        "    # if i['image']!=temp:\n",
        "    #   image = Image.open(os.path.join('images/', i['image'])).convert(\"RGB\")\n",
        "    #   images.append(preprocess(image))\n",
        "    #   temp=i['image']\n",
        "    # else:\n",
        "    #   images.append(images[-1])\n",
        "\n",
        "\n",
        "    # ocr.append(i['OCR'])\n",
        "    # entity.append(i['entity'])\n",
        "    labels.append(i['label'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(labels))\n",
        "print(labels[9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V2Hgp2xUUaZ",
        "outputId": "6a231ba8-570b-4986-9792-359f1906f11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1246\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"uspoliticsvallabels.pickle\", 'wb') as f:\n",
        "    pickle.dump(labels, f)\n"
      ],
      "metadata": {
        "id": "n5bCChDMTlyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5z1aNXSa-rF",
        "outputId": "4953e90f-be49-47ac-b8ee-a7ee855fc6e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1246"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ocr))\n",
        "print(len(entity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-u4AEAVRDrY",
        "outputId": "574b9f72-e0f1-494c-b633-8e356b436940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1246\n",
            "1246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open(\"trainimagesallrn50x64.pickle\", 'wb') as f:\n",
        "#     pickle.dump(images, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bj0khXBpRDof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"trainimagesallrn50x64.pickle\", 'rb') as f:\n",
        "#     images = pickle.load(f)"
      ],
      "metadata": {
        "id": "p3AmvoFjgGzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_input=torch.empty(7234,3,224,224)\n",
        "\n",
        "# def generator(k=50):\n",
        "#   n=0\n",
        "#   for i in data_new:\n",
        "#     if i['image'] not in os.listdir('images'):\n",
        "#         continue\n",
        "\n",
        "\n",
        "#     image = Image.open(os.path.join('images/', i['image'])).convert(\"RGB\")\n",
        "\n",
        "\n",
        "#        #image_input = torch.tensor(np.stack(images)).cuda()\n",
        "#      #original_images.append(image)\n",
        "#     images.append(preprocess(image))\n",
        "#     # n+=1\n",
        "    # if n%k==0 and n!=0:\n",
        "    #   temp = np.stack(images[n-k:n])\n",
        "    #   yield torch.tensor(temp)#.cuda()\n",
        "    #   #image_input = torch.cat((image_input.cuda(),))\n",
        "    # elif n>7200:\n",
        "    #   #image_input = torch.cat((image_input.cuda(),torch.tensor(np.stack(images[n:]).cuda()))).cuda()\n",
        "    #   yield torch.tensor(np.stack(images[n:])).cuda()\n",
        "    #   break\n",
        "\n",
        " # image_input=torch.tensor(image_input).cuda()"
      ],
      "metadata": {
        "id": "r6FocYpCVKGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# l=0\n",
        "# for x in generator():\n",
        "#   torch.cuda.empty_cache()\n",
        "#   l+=1\n",
        "#   print(l)\n",
        "#   image_input = torch.cat((image_input,x)).cuda()\n"
      ],
      "metadata": {
        "id": "mhmgQuEQcHN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "id": "8y0EARde6Iq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94f7283-dc3d-4ab4-87d0-0f63680f0ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10280"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#images[0].shape"
      ],
      "metadata": {
        "id": "xfbP91nqxs0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ei2Exe-fyVsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBgCanxi8JKw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#image_input = torch.tensor(np.stack(images)).cuda()\n",
        "\n",
        "# ocr_tokens = clip.tokenize([ o for o in ocr],truncate=True).cuda()\n",
        "# entity_tokens = clip.tokenize([ e for e in entity]).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSIn9V1d57Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "image_features = []#torch.Tensor(3000,640)\n",
        "for i in range(124):\n",
        "\n",
        "  image_input = torch.tensor(np.stack(images[k:k+10])).cuda()\n",
        "  k+=10\n",
        "  with torch.no_grad():\n",
        "    image_features=image_features + model.encode_image(image_input).float().tolist()\n",
        "    #image_features = torch.cat(image_features, model.encode_image(image_input).float())\n",
        "\n",
        "image_input = torch.tensor(np.stack(images[k:])).cuda()\n",
        "with torch.no_grad():\n",
        "    image_features=image_features + model.encode_image(image_input).float().tolist()"
      ],
      "metadata": {
        "id": "ZqFfb9OELhxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "# training_data = (image_input,ocr_tokens,entity_tokens)\n",
        "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "24Z3dhVryuPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JM2a41_Z12BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_features =[]\n",
        "# ocr_features=[]\n",
        "# entity_features=[]\n",
        "\n",
        "# epochs=10\n",
        "# i=0\n",
        "# while (i<len(image_input)):\n",
        "\n",
        "#         if (i+100)<len(image_input):\n",
        "#           image_input,ocr_tokens,entity_tokens =image_input[i:i+100].cuda(),ocr_tokens[i:i+100].cuda(),entity_tokens[i:i+100].cuda()\n",
        "\n",
        "#         if (i+100)>=len(image_input):\n",
        "#           image_input,ocr_tokens,entity_tokens =image_input[i:].cuda(),ocr_tokens[i:].cuda(),entity_tokens[i:].cuda()\n",
        "#         i=i+100\n",
        "\n",
        "#           #images, labels = images.cuda(), labels.cuda()\n",
        "#         with torch.no_grad():\n",
        "\n",
        "#           image_features.append(model.encode_image(image_input).float())\n",
        "#           ocr_features.append(model.encode_text(ocr_tokens).float())\n",
        "#           entity_features.append(model.encode_text(entity_tokens).float())\n"
      ],
      "metadata": {
        "id": "wnks7vvRx5Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "# ocr_tokens = clip.tokenize([ o for o in ocr],truncate=True).cuda()\n",
        "# entity_tokens = clip.tokenize([ e for e in entity]).cuda()\n",
        "ocr_features = []\n",
        "entity_features = []\n",
        "for i in range(124):\n",
        "\n",
        "  o = clip.tokenize(ocr[k:k+10],truncate=True).cuda()\n",
        "  e = clip.tokenize(entity[k:k+10]).cuda()\n",
        "\n",
        "  k+=10\n",
        "  with torch.no_grad():\n",
        "    ocr_features=ocr_features + model.encode_text(o).float().tolist()\n",
        "    entity_features=entity_features + model.encode_text(e).float().tolist()\n",
        "    #image_features = torch.cat(image_features, model.encode_image(image_input).float())\n",
        "\n",
        "o = clip.tokenize(ocr[k:],truncate=True).cuda()\n",
        "e = clip.tokenize(entity[k:]).cuda()\n",
        "with torch.no_grad():\n",
        "    ocr_features=ocr_features + model.encode_text(o).float().tolist()\n",
        "    entity_features=entity_features + model.encode_text(e).float().tolist()"
      ],
      "metadata": {
        "id": "TWAresARwKpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ocr_features))\n",
        "print(len(entity_features))"
      ],
      "metadata": {
        "id": "DSvNTevtpqoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a02102c-d38c-490d-ba93-e46323b09286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1246\n",
            "1246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN9I0nIBZ_vW"
      },
      "outputs": [],
      "source": [
        "# ocr_features = []\n",
        "# entity_features = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     #image_features= model.encode_image(image_input).float()\n",
        "#     ocr_features = model.encode_text(ocr_tokens).float()\n",
        "#     entity_features = model.encode_text(entity_tokens).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V6QLh1PVCu4"
      },
      "source": [
        "**Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_features= torch.Tensor(image_features).cuda()\n",
        "ocr_features= torch.Tensor(ocr_features).cuda()\n",
        "entity_features= torch.Tensor(entity_features).cuda()"
      ],
      "metadata": {
        "id": "asSPchX5T_Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(image_features))\n",
        "print(type(ocr_features))\n",
        "type(entity_features)\n",
        "type(image_features)"
      ],
      "metadata": {
        "id": "WScEVMqqO6qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691a87cc-b183-4b6e-9dcd-746b3a00f8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McdKjU0UVEUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8881fccc-137a-44d4-cb4a-5fb3d7cb0b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1246\n",
            "1246\n",
            "1246\n",
            "1246\n"
          ]
        }
      ],
      "source": [
        "print(len(image_features))\n",
        "print(len(ocr_features))\n",
        "print(len(entity_features))\n",
        "\n",
        "print(len(labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKTIjifwPCzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4f9ead-c2cb-4368-816d-075702c8e2a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768])\n",
            "torch.Size([768])\n",
            "torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "print((image_features[0]).shape)\n",
        "print(ocr_features[0].shape)\n",
        "print(entity_features[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4Ec1zcZq9hD"
      },
      "outputs": [],
      "source": [
        "print(ocr_features[0])\n",
        "print(entity_features[0])\n",
        "print(image_features[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ocr_features.is_cuda)\n",
        "print(entity_features.is_cuda)\n",
        "print(image_features.is_cuda)\n",
        "#image_features= image_features.cuda()\n",
        "print(image_features.is_cuda)"
      ],
      "metadata": {
        "id": "A5VNDPr2Uf5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d601fd5-2a92-4db8-87aa-165848c70ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3HxhR6jhn-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yfY-yKTPhocd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gku4Sh2ihlFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jGagYVkuhzYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c8yCKn_8lSem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtKFd-3Hhk-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQ6HILE3jmkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "concatenating"
      ],
      "metadata": {
        "id": "fpCV7q_Ch55q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.cat((image_features, ocr_features, entity_features), 1)\n"
      ],
      "metadata": {
        "id": "8BUG5UBd4UV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfmVZNQme6Lv",
        "outputId": "bd6884f8-da2b-46b3-8d7e-e9399f218843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1246, 2304])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving training embeddings"
      ],
      "metadata": {
        "id": "faZVWhRRGIwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"uspoliticsViT-L14valembed.pickle\", 'wb') as f:\n",
        "    pickle.dump(input_tensor, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "RLUP5sQgCyi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open(\"finaltrainembed.pickle\", 'rb') as f:\n",
        "#     x = pickle.load(f)"
      ],
      "metadata": {
        "id": "sEvOEm-8F9Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "nsHANq11KbKL",
        "outputId": "cebdbc43-fdb6-491d-a127-11c39fa45e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-9f2b259887ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_tensor))\n",
        "print(input_tensor.shape)\n",
        "print(input_tensor)"
      ],
      "metadata": {
        "id": "AtKEgURt7d-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying clip**"
      ],
      "metadata": {
        "id": "kAPC07VlW-xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in test_tensor[:2]:\n",
        "\n",
        "#   print(i)"
      ],
      "metadata": {
        "id": "BaBVgZPCa-DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yfinal = []\n",
        "# for i in test_tensor:\n",
        "#   y = [sum(value) for value in zip(i[:512], i[512:512*2],i[512*2:])]\n",
        "#   y = [x/3 for x in y]\n",
        "#   yfinal.append(torch.Tensor(y).cuda())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cGHUJwl7W-Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yfinal =torch.Tensor(yfinal)"
      ],
      "metadata": {
        "id": "Q6gqYSJLcG7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_tensor.type"
      ],
      "metadata": {
        "id": "HoTr3ePsal3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(yfinal)):\n",
        "#    yfinal[i] = yfinal[i].to(device)\n",
        "\n",
        "# print(yfinal[7])"
      ],
      "metadata": {
        "id": "EkC4z7P_ZlwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "# qw =['hero','villian','victim','other']\n",
        "# diffpred = []\n",
        "# txt = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in qw]).to(device)\n",
        "\n",
        "# # Calculate features\n",
        "# with torch.no_grad():\n",
        "\n",
        "#     txtfea = model.encode_text(txt).float()\n",
        "# for i in range(1290):\n",
        "#       im =yfinal[i]\n",
        "#       # Pick the top 5 most similar labels for the image\n",
        "#       im /= im.norm(dim=-1, keepdim=True)\n",
        "#       txtfea /= txtfea.norm(dim=-1, keepdim=True)\n",
        "#       similarity = (100.0 * im @ txtfea.T).softmax(dim=-1)\n",
        "#       values, indices = similarity.topk(1)\n",
        "#       index =torch.Tensor.cpu(indices).numpy()\n",
        "#       diffpred.append(int(index))\n",
        "# # Print the result\n",
        "# # print(\"\\nTop predictions:\\n\")\n",
        "# # for value, index in zip(values, indices):\n",
        "# #     print(f\"{qw[index]}: {100 * value.item():.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xwoyD1qKP8Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(diffpred))"
      ],
      "metadata": {
        "id": "S5M9Sv4FV7us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(testlabels))"
      ],
      "metadata": {
        "id": "GSKf8iymRLt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f1_score(testlabels, diffpred, average='macro')"
      ],
      "metadata": {
        "id": "1LmFOTS9feKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open(\"bert1000trainembeddings.pickle\", 'rb') as f:\n",
        "#     bert_train_embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "JkPPU_7AsUlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_train_embeddings = torch.Tensor(bert_train_embeddings).cuda()"
      ],
      "metadata": {
        "id": "IsKWy95_s_Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open(\"bert1000entitytrainembeddings.pickle\", 'rb') as f:\n",
        "#     bert_entity_train_embeddings = pickle.load(f)\n",
        "\n",
        "# bert_entity_train_embeddings = torch.Tensor(bert_entity_train_embeddings).cuda()"
      ],
      "metadata": {
        "id": "ByHNmBahW54m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBcql1rHW5zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_tensor[:1000].shape"
      ],
      "metadata": {
        "id": "4Dmg_C5fxDT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_train_embeddings.shape"
      ],
      "metadata": {
        "id": "Q-3wXmNQxMu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_entity_train_embeddings.shape"
      ],
      "metadata": {
        "id": "C7akT1tKXM68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_tensor = torch.cat((input_tensor[:1000],bert_train_embeddings,bert_entity_train_embeddings), 1)"
      ],
      "metadata": {
        "id": "_-in0PGUsHlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.listdir()\n",
        "# !mkdir 'test_data'\n",
        "# !unzip 'constraint22_dataset_unseen_test.zip' -d 'test_data'"
      ],
      "metadata": {
        "id": "IjWn9gAG-oEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing test embeddings\n"
      ],
      "metadata": {
        "id": "DWzGc4wqGhQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "kqQZ0Gvn_ii8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd4c9f8-07d2-4de3-c92d-4b8032f1448e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'constraint22_dataset_covid19.zip',\n",
              " 'annotations',\n",
              " 'images',\n",
              " 'constraint22_dataset_uspolitics.zip',\n",
              " 'constraint22_dataset_unseen_test.zip',\n",
              " 'test_data',\n",
              " 'constraint22_dataset_unseen_test_gold_labels.jsonl',\n",
              " 'trainimagesallrn50x64.pickle',\n",
              " 'trainembeddings.pickle',\n",
              " 'bert1000trainembeddings.pickle',\n",
              " 'bert1000entitytrainembeddings.pickle',\n",
              " 'uspoliticsdata',\n",
              " 'testembeddings.pickle',\n",
              " 'testlabels.pickle',\n",
              " 'berttestembeddings.pickle',\n",
              " 'berttestentityembeddings.pickle',\n",
              " 'bertentitytrainembeddings.pickle',\n",
              " 'berttrainembeddings.pickle',\n",
              " 'trainlabels.pickle',\n",
              " 'finaltrainembed.pickle',\n",
              " 'ViT-B16trainembed.pickle',\n",
              " 'ViT-L14trainembed.pickle',\n",
              " 'RN50trainembed.pickle',\n",
              " 'RN101trainembed.pickle',\n",
              " 'RN50x4trainembed.pickle',\n",
              " 'RN50x16trainembed.pickle',\n",
              " 'ViT-B16testembeddings.pickle',\n",
              " 'ViT-L14testembeddings.pickle',\n",
              " 'RN50testembeddings.pickle',\n",
              " 'RN101testembeddings.pickle',\n",
              " 'RN50x4testembeddings.pickle',\n",
              " 'RN50x16testembeddings.pickle',\n",
              " 'uspoliticsfinaltrainembed.pickle',\n",
              " 'uspoliticstestembed.pickle',\n",
              " 'uspoliticslabels.pickle',\n",
              " 'uspoliticstestlabels.pickle',\n",
              " 'uspoliticsViT-B16trainembed.pickle',\n",
              " 'uspoliticsViT-B16testembed.pickle',\n",
              " 'uspoliticsViT-L14trainembed.pickle',\n",
              " 'uspoliticsViT-L14testembed.pickle',\n",
              " 'uspoliticsRN50trainembed.pickle']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('constraint22_dataset_unseen_test_gold_labels.jsonl') as f:\n",
        "    testdata = [json.loads(line) for line in f]\n",
        "print(testdata[4])"
      ],
      "metadata": {
        "id": "me4M9fkhAApj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d76276-0e68-4fcd-a242-67035756b068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'OCR': 'TOGETHER THEY RAPED A 13 YR OLD GIRL\\nTHEY ARE BACKIN COURT DECEMBER 16/2016\\n', 'image': 'memes_1731.png', 'hero': [], 'villain': ['jeffrey epstein', 'donald trump'], 'victim': ['13 year old girl'], 'other': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(testdata))"
      ],
      "metadata": {
        "id": "TvxWU8w7loML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752470d6-687e-4deb-d0f7-8795fadca43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = []\n",
        "k=0\n",
        "for i in testdata :\n",
        "  i['entities']=i['hero']+i['villain']+i['victim']+i['other']\n",
        "\n",
        "  if i['image'].startswith('m'):\n",
        "    for en in i['entities']:\n",
        "      new_dict={}\n",
        "\n",
        "      new_dict['image'] = i['image']\n",
        "      new_dict['OCR'] = i['OCR']\n",
        "      new_dict['entity'] = en\n",
        "      if en in i['hero']:\n",
        "        new_dict['label'] =0 #'hero'\n",
        "      elif en in i['villain'] :\n",
        "        new_dict['label'] =1 #'villain'\n",
        "      elif en in i['victim'] :\n",
        "        new_dict['label'] =2 #'victim'\n",
        "      else:\n",
        "        new_dict['label'] =3 #'other'\n",
        "\n",
        "\n",
        "      test.append(new_dict)\n",
        "  k=k+1"
      ],
      "metadata": {
        "id": "HsHwM8LABU_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[5])\n",
        "print(k)"
      ],
      "metadata": {
        "id": "l34UypE_Bui1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cdaf82-f3c5-4a5e-e85e-bba121e0ca9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image': 'memes_3330.png', 'OCR': 'APPARENTLY\\nCAKES\\nARE MORE IMPORTANT\\nTOYOU THANALLOFTHIS:\\nTAIX\\nSTOPPING BAANE STRIKIS INUING THE FOICE STATE ELMINATING TLES\\nBRING\\nTHE\\nTROOPS\\nHOME\\nSudget\\nNOW\\nLEGALIZE\\nINDING THE\\nBINGING OR\\nTROOPS OME\\nALANCIN\\nTHE UDG\\nLERT RIGHTS\\nEND\\nTHESE\\nWARS\\nDIT TE D\\nSIPAnON Or\\nCHURCHA STATE\\nNO MORE\\nCON CONTROL\\nWARS\\n', 'entity': 'people', 'label': 3}\n",
            "718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testimages = []\n",
        "testocr = []\n",
        "testentity=[]\n",
        "testlabels = []\n",
        "\n",
        "\n",
        "temp=''\n",
        "for i in test:\n",
        "  if i['image'] not in os.listdir('test_data/images'):\n",
        "        continue\n",
        "  testocr.append(i['OCR'])\n",
        "  testentity.append(i['entity'])\n",
        "  testlabels.append(i['label'])\n",
        "\n",
        "\n",
        "\n",
        "  if i['image']!=temp:\n",
        "      image = Image.open(os.path.join('test_data/images/', i['image'])).convert(\"RGB\")\n",
        "      testimages.append(preprocess(image))\n",
        "      temp=i['image']\n",
        "  else:\n",
        "      testimages.append(testimages[-1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #testocr.append(i['OCR'])\n",
        "  # testentity.append(i['entity'])\n",
        "  # testlabels.append(i['label'])\n",
        "\n",
        "# n=0\n",
        "\n",
        "# for i in data_new:\n",
        "#     n+=1\n",
        "#     if n==3001:\n",
        "#       break\n",
        "\n",
        "#     if i['image'] not in os.listdir('images'):\n",
        "#         continue\n",
        "\n",
        "\n",
        "#     if i['image']!=temp:\n",
        "#       image = Image.open(os.path.join('images/', i['image'])).convert(\"RGB\")\n",
        "#       images.append(preprocess(image))\n",
        "#       temp=i['image']\n",
        "#     else:\n",
        "#       images.append(images[-1])"
      ],
      "metadata": {
        "id": "JXZBIoFBBBwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(testocr)"
      ],
      "metadata": {
        "id": "3f_ovOEkX4eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e50b0b-72ac-4b05-f470-99ed97a5545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1143"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(testimages)"
      ],
      "metadata": {
        "id": "vjxPmvK9WBn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd655ea3-5054-4aec-df5b-bbc66613f38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1143"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(testentity.shape)"
      ],
      "metadata": {
        "id": "DPHb1_o9s_ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=0\n",
        "testimage_features = []#torch.Tensor(3000,640)\n",
        "for i in range(381):\n",
        "\n",
        "  testimage_input = torch.tensor(np.stack(testimages[k:k+3])).cuda()\n",
        "  k+=3\n",
        "  with torch.no_grad():\n",
        "    testimage_features=testimage_features + model.encode_image(testimage_input).float().tolist()"
      ],
      "metadata": {
        "id": "ZXvsPMFOWA2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testimage_input = torch.tensor(np.stack(testimages)).cuda()\n",
        "testocr_tokens = clip.tokenize([ o for o in testocr],truncate=True).cuda()\n",
        "testentity_tokens = clip.tokenize([ e for e in testentity]).cuda()"
      ],
      "metadata": {
        "id": "CjtUzgv1AAm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    #testimage_features= model.encode_image(testimage_input).float()\n",
        "    testocr_features = model.encode_text(testocr_tokens).float()\n",
        "    testentity_features = model.encode_text(testentity_tokens).float()"
      ],
      "metadata": {
        "id": "breK8QmxDh-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(testimage_features)\n",
        "print(testocr_features.shape)\n",
        "print(testentity_features.shape)"
      ],
      "metadata": {
        "id": "fV4a_9tMtNZb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd29fff-85d8-44e5-e45e-1bfa30c217fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1143, 768])\n",
            "torch.Size([1143, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testimage_features=torch.Tensor(testimage_features).cuda()"
      ],
      "metadata": {
        "id": "qiOJt9jjWk5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor = torch.cat((testimage_features, testocr_features, testentity_features), 1)"
      ],
      "metadata": {
        "id": "r0MHzmspDnbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_tensor))\n",
        "print(len(testlabels))"
      ],
      "metadata": {
        "id": "pWqh73xiIjXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94db369a-47b8-4294-8a40-cd73d1a9a542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1143\n",
            "1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# size=len(input_tensor)\n",
        "# xtrain=input_tensor[:int(0.7*size),]\n",
        "# xval=input_tensor[int(0.7*size):,]\n",
        "# ytrain=labels[:int(0.7*size)]\n",
        "# yval=labels[int(0.7*size):]"
      ],
      "metadata": {
        "id": "M1H5Mdtj8uvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(testlabels))"
      ],
      "metadata": {
        "id": "8fBiK2r9lTjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d691dbed-54b3-442e-fce6-7e1798ecde95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"uspoliticsRN50x16testembed.pickle\", 'wb') as f:\n",
        "    pickle.dump(test_tensor, f)\n",
        "\n",
        "# import pickle\n",
        "# with open(\"uspoliticslabels.pickle\", 'wb') as f:\n",
        "#     pickle.dump(labels, f)\n",
        "\n",
        "# import pickle\n",
        "# with open(\"uspoliticstestlabels.pickle\", 'wb') as f:\n",
        "#     pickle.dump(testlabels, f)"
      ],
      "metadata": {
        "id": "UFlWl-9GLodb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating final data into train val test for classification**"
      ],
      "metadata": {
        "id": "iZKyliY_tWjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xtrain=input_tensor\n",
        "xval=test_tensor\n",
        "ytrain=labels\n",
        "yval=testlabels\n",
        "\n",
        "\n",
        "# xtrain=entity_features\n",
        "# xval=testentity_features\n",
        "# ytrain=labels\n",
        "# yval=testlabels"
      ],
      "metadata": {
        "id": "Ah42_IptEER2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(xtrain.shape)\n",
        "print(xval.shape)\n",
        "#xtrain=xtrain.numpy()\n",
        "print(type(xtrain))\n",
        "print(len(ytrain))\n",
        "#print(yval.shape)"
      ],
      "metadata": {
        "id": "-dn4FQqHm9rN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940b6b84-4e85-4843-891a-7979a9ca38d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10280, 1536])\n",
            "torch.Size([1143, 1536])\n",
            "<class 'torch.Tensor'>\n",
            "10280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "49lholMAMKEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQlkGq6HkjnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azkvQSLDkjj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c1hzDMUZkvYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-MAnJNM8NqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62zJh6FId5cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_F5higs-eNor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qAW8nZEpeHec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTm5Bfz5YcI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vW207WyMlh5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max=0\n",
        "# for i in range(len(target_entity_embeddings)):\n",
        "\n",
        "#   if len(target_entity_embeddings[i])>max:\n",
        "#     max=len(target_entity_embeddings[i])\n",
        "# print(max)\n",
        "\n",
        "\n",
        "# for i in range(len(target_entity_embeddings)):\n",
        "#   if len(target_entity_embeddings[i]) <9216:\n",
        "#     target_entity_embeddings[i] = target_entity_embeddings[i] + [0]*(9216-len(target_entity_embeddings[i]))"
      ],
      "metadata": {
        "id": "r5bbuRrDnuA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TCI6H0dw6Qnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5aN06CCzrJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LtJ96wU_67HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IulQMckhntCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "biYWluqHn2gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLnk97BjlhxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "HTBm_Jdin_9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAPv7P0bgGq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33742665-3e42-4eff-b10c-1cf2689c583a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_tensor = torch.cat((testimage_features, testocr_features, testentity_features,target_embeddings,target_entity_embeddings), 1)"
      ],
      "metadata": {
        "id": "BOqYqMEAgOWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Models"
      ],
      "metadata": {
        "id": "C_UiUi6q8vyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "aD8d_mf1RXBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k neighbors classifier"
      ],
      "metadata": {
        "id": "AJhM2OnowvFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=4)\n",
        "xtrain2 = torch.Tensor.cpu(xtrain)\n",
        "\n",
        "\n",
        "neigh.fit(xtrain2, ytrain)"
      ],
      "metadata": {
        "id": "H64RM-MNrc7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1b6201-5568-4ca7-c2e6-ebfceb4f3583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=4)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XD_LSFKjvDIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xval2 = torch.Tensor.cpu(xval)"
      ],
      "metadata": {
        "id": "T34Lic9hugRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypredict = neigh.predict(xval2)"
      ],
      "metadata": {
        "id": "ajYxyXzyvt-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(ypredict==yval))/len(yval)"
      ],
      "metadata": {
        "id": "YPrLtBPdwaE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a533b79-ae30-403f-a00e-391e1026acb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.689413823272091"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(yval, ypredict, average='macro')"
      ],
      "metadata": {
        "id": "VZN_34pqwVp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896874bf-1312-4716-afd2-b3f2d08b7b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45554355261073987"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ypredict)\n",
        "print(yval)\n"
      ],
      "metadata": {
        "id": "QD-zHr2ajVsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209ccceb-fd70-4b35-c6a3-92553be8971c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 3 3 ... 3 3 1]\n",
            "[2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 0, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 3, 3, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 2, 3, 1, 3, 0, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 2, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 1, 1, 1, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 0, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 2, 3, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 0, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 0, 3, 3, 2, 2, 2, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 1, 3, 1, 3, 2, 3, 3, 3, 3, 0, 1, 1, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 2, 3, 0, 0, 0, 1, 1, 1, 0, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 0, 3, 3, 1, 2, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 0, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 0, 2, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 1, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "decision tree\n"
      ],
      "metadata": {
        "id": "MNvR8Ydtxm1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(xtrain2, ytrain)"
      ],
      "metadata": {
        "id": "z27BNdFAv0d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ypred=clf.predict(xval2)\n"
      ],
      "metadata": {
        "id": "nJrJZnD7rcyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(ypred==yval))/len(yval)"
      ],
      "metadata": {
        "id": "2K3IOBDprclu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fc738b-ca69-4f3e-f0a3-b2c46a28712e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7730528200537153"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(yval, ypred, average='macro')"
      ],
      "metadata": {
        "id": "5956TqRBxY5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9ba2cdb-d2e4-4b3b-a68c-ca1587bc3535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3462627835132901"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent"
      ],
      "metadata": {
        "id": "dKQB8YrGyTPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "cl = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
        "cl.fit(xtrain2, ytrain)\n",
        "SGDClassifier(max_iter=5)"
      ],
      "metadata": {
        "id": "9LSQKmOjxY3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb83ac0-acd0-41c8-d4ca-f48b5fc50526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(max_iter=5)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypre = cl.predict(xval2)"
      ],
      "metadata": {
        "id": "SaMiYtiSx6Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(ypre==yval))/len(yval)"
      ],
      "metadata": {
        "id": "FHd8dofox6HR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82fe3edf-bba9-462a-c9aa-fbb5f62654d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549686660698299"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(yval, ypre, average='macro')"
      ],
      "metadata": {
        "id": "IGqWyQvjSsaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cabd5ac-59ba-42b5-8075-8f022d2340f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30040069686411147"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "i-ouUHrt3hP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC()\n",
        "clf.fit(xtrain2, ytrain2)"
      ],
      "metadata": {
        "id": "Yv4Yj8Cc2sGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e50fe89-0d90-48e3-8fdf-492c615ecf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = clf.predict(xval2)\n",
        "(sum(ypred==yval))/len(yval)"
      ],
      "metadata": {
        "id": "ZX8BneKM2sB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "a0b4736a-ba0b-4ae9-954e-0ca14dd61e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-544035960b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxval2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(yval, ypred, average='macro')"
      ],
      "metadata": {
        "id": "ITLxzW5k2r5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83356797-fc3d-4d43-bd20-0651dfe3a457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22864955826672279"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9HKpab72r29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kmOTokdOlZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7NojuBQOlW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "evMyDKxnOlUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ydd4Vna8OlRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jewjAR7mOjBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SBtXRZ44Oi-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zd8eNTsFOi5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6m0uJA83Oi3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very basic NN"
      ],
      "metadata": {
        "id": "gSLJgFgf8oMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain =torch.tensor(ytrain).cuda()\n",
        "yval =torch.tensor(yval).cuda()\n",
        "xtrain =torch.tensor(xtrain).cuda()\n",
        "xval =torch.tensor(xval).cuda()"
      ],
      "metadata": {
        "id": "VEkxQ5KXx6E0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a1ba6a4-0f9a-4b71-fc14-6fb133eb41e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESwG95fL-T56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "input_size = 1536\n",
        "hidden_size = 512\n",
        "num_classes = 4\n",
        "num_epochs = 16\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "train_dataset = TensorDataset(xtrain,ytrain)\n",
        "\n",
        "test_dataset = TensorDataset(xval,yval)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "\n",
        "        # no activation and no softmax at the end\n",
        "        return out\n",
        "\n",
        "nnmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(nnmodel.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss=0.0\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    for data, trainlabels in train_loader:#xtrain.cuda(), ytrain.cuda()\n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward Pass\n",
        "        target = nnmodel(data)\n",
        "        # Find the Loss\n",
        "        loss = criterion(target,trainlabels)\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "        # Update Weights\n",
        "        optimizer.step()\n",
        "        # Calculate Loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "  print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / n_total_steps} \\t\\t Validation Loss: {train_loss / len(test_loader)}')\n",
        "\n",
        "\n",
        "#nnmodel()\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "prediction =[]\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        testdata, tralabels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = nnmodel(testdata)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        prediction.append(predicted)\n",
        "        total += tralabels.size(0)\n",
        "        correct += (predicted == tralabels).sum().item()\n",
        "\n",
        "    # for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "    #     images = images.to(device)\n",
        "    #     labels = labels.to(device)\n",
        "\n",
        "    #     # Forward pass\n",
        "    #     outputs = model(images)\n",
        "    #     loss = criterion(outputs, labels)\n",
        "\n",
        "    #     # Backward and optimize\n",
        "    #     optimizer.zero_grad()\n",
        "    #     loss.backward()\n",
        "    #     optimizer.step()\n",
        "\n",
        "        # if (i+1) % 100 == 0:\n",
        "        #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "WIHY7TZp899X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49389160-2bc9-4167-edff-1f2ccbc018a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 0.638728417039658 \t\t Validation Loss: 5.713070841299163\n",
            "Epoch 2 \t\t Training Loss: 0.5329343032855425 \t\t Validation Loss: 4.766801268276241\n",
            "Epoch 3 \t\t Training Loss: 0.45921239724255497 \t\t Validation Loss: 4.107399775336186\n",
            "Epoch 4 \t\t Training Loss: 0.39861265525962253 \t\t Validation Loss: 3.5653687498221793\n",
            "Epoch 5 \t\t Training Loss: 0.33410449321410673 \t\t Validation Loss: 2.9883790781928434\n",
            "Epoch 6 \t\t Training Loss: 0.2787450464060588 \t\t Validation Loss: 2.4932195817430816\n",
            "Epoch 7 \t\t Training Loss: 0.2346639286101975 \t\t Validation Loss: 2.098938472568989\n",
            "Epoch 8 \t\t Training Loss: 0.1843194063313259 \t\t Validation Loss: 1.6486346899635262\n",
            "Epoch 9 \t\t Training Loss: 0.15074950491905398 \t\t Validation Loss: 1.3483705717759829\n",
            "Epoch 10 \t\t Training Loss: 0.13042282483176046 \t\t Validation Loss: 1.1665597109951906\n",
            "Epoch 11 \t\t Training Loss: 0.11302801716482695 \t\t Validation Loss: 1.0109728201965078\n",
            "Epoch 12 \t\t Training Loss: 0.09535657159654388 \t\t Validation Loss: 0.8529115570579759\n",
            "Epoch 13 \t\t Training Loss: 0.08015503426898063 \t\t Validation Loss: 0.7169422509614378\n",
            "Epoch 14 \t\t Training Loss: 0.07181519678481478 \t\t Validation Loss: 0.6423470379086211\n",
            "Epoch 15 \t\t Training Loss: 0.06809352312572532 \t\t Validation Loss: 0.6090587346245431\n",
            "Epoch 16 \t\t Training Loss: 0.06267859249503022 \t\t Validation Loss: 0.5606251884277703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = prediction[0]\n",
        "k=0\n",
        "for i in prediction:\n",
        "\n",
        "  predictions = torch.cat((predictions,i),0)\n",
        "  k=k+1"
      ],
      "metadata": {
        "id": "y4CDOBUqByHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((predictions[:]))\n",
        "predictions =predictions[32:]\n",
        "print(len(predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K91eleACDYJU",
        "outputId": "e8f172ce-4302-4098-82c3-1f40e26a6441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3,  ..., 3, 3, 3], device='cuda:0')\n",
            "1143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = predictions.cpu()\n",
        "yval =yval.cpu()\n"
      ],
      "metadata": {
        "id": "iH78wMxQEvU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(yval, ypred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1J8YewGEfOa",
        "outputId": "234d9a4a-e58e-4b4e-cd4d-ec9fd62f0d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48433538760315487"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFu-F35-Bc2U",
        "outputId": "d22dcb89-e321-43de-9929-b1ccee769841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 0, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
            "        3, 1, 3, 0, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 0, 0, 3, 0, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 2, 3], device='cuda:0'), tensor([3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3,\n",
            "        3, 3, 3, 3, 3, 1, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 1, 1, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 2, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
            "        1, 3, 3, 3, 2, 1, 3, 3], device='cuda:0'), tensor([3, 3, 1, 3, 3, 3, 1, 0, 3, 0, 3, 3, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 1, 1, 1, 1, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 1, 3, 1, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), tensor([3, 1, 1, 1, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3,\n",
            "        3, 3, 1, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1,\n",
            "        3, 1, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 2, 3,\n",
            "        3, 3, 3, 3, 1, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 2, 2, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), tensor([3, 3, 3, 3, 2, 2, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 1, 2, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), tensor([1, 3, 3, 1, 2, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3,\n",
            "        1, 3, 3, 1, 3, 2, 3, 3], device='cuda:0'), tensor([2, 1, 3, 1, 2, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 1], device='cuda:0'), tensor([1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 1, 3, 3, 1, 1, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
            "        1, 1, 1, 3, 3, 1, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0'), tensor([0, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        1, 3, 3, 1, 3, 3, 3, 2], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 1, 3, 3, 2, 3, 3], device='cuda:0'), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3], device='cuda:0')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(prediction[-2]))"
      ],
      "metadata": {
        "id": "E1nZ_hik6-QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5657c6a-e688-4781-f1fe-17a6ee60e986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(40*32+10)"
      ],
      "metadata": {
        "id": "bVLv6dByq16d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106e4159-bb99-4bd1-aa1f-57a75987f025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuxm2Gt4Wvzt"
      },
      "source": [
        "## Calculating cosine similarity\n",
        "\n",
        "We normalize the features and calculate the dot product of each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAxkQR7bf3A"
      },
      "outputs": [],
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OozrhlEVL6w"
      },
      "outputs": [],
      "source": [
        "print(similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQz5hEtVLWi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNMhhIeGONhw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5zvMxh8cU6m"
      },
      "outputs": [],
      "source": [
        "count = len(descriptions)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "# plt.colorbar()\n",
        "plt.yticks(range(count), texts, fontsize=18)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alePijoXy6AH"
      },
      "source": [
        "# Zero-Shot Image Classification\n",
        "\n",
        "You can classify images using the cosine similarity (times 100) as the logits to the softmax operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqu4GlfPfr-p"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), transform=preprocess, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4S__zCGy2MT"
      },
      "outputs": [],
      "source": [
        "text_descriptions = [f\"This is a photo of a {label}\" for label in cifar100.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4z1fm9vCpSR"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Ju_6IBE2Iz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "for i, image in enumerate(original_images):\n",
        "    plt.subplot(4, 4, 2 * i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(4, 4, 2 * i + 2)\n",
        "    y = np.arange(top_probs.shape[-1])\n",
        "    plt.grid()\n",
        "    plt.barh(y, top_probs[i])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.gca().set_axisbelow(True)\n",
        "    plt.yticks(y, [cifar100.classes[index] for index in top_labels[i].numpy()])\n",
        "    plt.xlabel(\"probability\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPePcUttFmFL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kAPC07VlW-xJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}